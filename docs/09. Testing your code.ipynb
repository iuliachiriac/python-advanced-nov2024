{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623f03e5",
   "metadata": {},
   "source": [
    "## Why should we test code?\n",
    "\n",
    "Testing your code is a very important step in the release cycle.\n",
    "\n",
    "Why is it important?\n",
    "1. Until you execute a line of code, you don't know if that line can work at all.\n",
    "1. Until you execute your code with a representative set of basic use cases, you don't know if the code can work end-to-end.\n",
    "1. If you and possibly other people are going to modify your code, it's very easy to break it in unexpected ways. A suite of automated unit and integration tests gives you confidence you've not broken anything significant.\n",
    "1. Tests can be used for profiling, to help you understand changes in your system's performance, and raise a flag if something degrades significantly.\n",
    "\n",
    "## Types of testing\n",
    "\n",
    "For any software application, both unit testing, as well as integration testing, is very important as each of them employs a unique process to test a software application.\n",
    "\n",
    "**Unit testing** means testing individual modules of an application in isolation (without any interaction with dependencies) to confirm that those pieces of code are doing things right. **Integration testing** means checking if different modules are working fine when combined together as a group\n",
    "\n",
    "There are all kinds of tests besides integration and unit tests, and they're all important.\n",
    "\n",
    "* Regression tests, to make sure no bugs have recurred.\n",
    "* Performance tests against various workloads, to characterize and guide improvement of performance.\n",
    "* Stress tests to make sure the software can handle high loads, and work on a busy system.\n",
    "* Resource tests, both to ensure resource consumption isn't unreasonable, and to ensure the code works as expected under low resource conditions.\n",
    "* Change testing, for example when the IP address changes, the time is changed on the box, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd3bde",
   "metadata": {},
   "source": [
    "## 1. Testing frameworks: `unittest`\n",
    "\n",
    "`unittest` is a Python Standard Librabry module which provides a rich set of tools for constructing and running tests.\n",
    "\n",
    "To achieve this, `unittest` supports some important concepts in an object-oriented way:\n",
    "\n",
    "* **test fixture**\n",
    "A test fixture represents the preparation needed to perform one or more tests, and any associate cleanup actions. This may involve, for example, creating temporary or proxy databases, directories, or starting a server process.\n",
    "\n",
    "* **test case**\n",
    "A test case is the individual unit of testing. It checks for a specific response to a particular set of inputs. unittest provides a base class, TestCase, which may be used to create new test cases.\n",
    "\n",
    "* **test suite**\n",
    "A test suite is a collection of test cases, test suites, or both. It is used to aggregate tests that should be executed together.\n",
    "\n",
    "* **test runner**\n",
    "A test runner is a component which orchestrates the execution of tests and provides the outcome to the user. The runner may use a graphical interface, a textual interface, or return a special value to indicate the results of executing the tests.\n",
    "\n",
    "\n",
    "The building block of a test is the test case. With `unittest`, you can create a test case by subclassing the `unittest.TestCase` class. Each method starting with `test` will be an individual testing scenario. Test methods should contain at least one `assert*` method call as this is the essential role of a test: comparing actual results agaist expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e962f4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_isupper (__main__.TestStringMethods.test_isupper) ... ok\n",
      "test_split (__main__.TestStringMethods.test_split) ... ok\n",
      "test_upper (__main__.TestStringMethods.test_upper) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x10314ca70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestStringMethods(unittest.TestCase):\n",
    "\n",
    "    def test_upper(self):\n",
    "        self.assertEqual('foo'.upper(), 'FOO')\n",
    "\n",
    "    def test_isupper(self):\n",
    "        self.assertTrue('FOO'.isupper())\n",
    "        self.assertFalse('Foo'.isupper())\n",
    "\n",
    "    def test_split(self):\n",
    "        s = 'hello world'\n",
    "        self.assertEqual(s.split(), ['hello', 'world'])\n",
    "        # check that s.split fails when the separator is not a string\n",
    "        with self.assertRaises(TypeError):\n",
    "            s.split(2)\n",
    "\n",
    "\n",
    "unittest.main(argv=['-k', 'TestStringMethods'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e46ef",
   "metadata": {},
   "source": [
    "The following table lists the most commonly used assert methods:\n",
    "\n",
    "\n",
    "| Method                                  | Checks that                       |\n",
    "| --------------------------------------- | --------------------------------- |\n",
    "| `assertEqual(a, b)`                     | `a == b`                          |\n",
    "| `assertNotEqual(a, b)`                  | `a != b`                          |\n",
    "| `assertTrue(x)`                         | `bool(x) is True`                 |\n",
    "| `assertFalse(x)`                        | `bool(x) is False`                |\n",
    "| `assertIs(a, b)`                        | `a is b`                          |\n",
    "| `assertIsNot(a, b)`                     | `a is not b`                      |\n",
    "| `assertIsNone(x)`                       | `x is None`                       |\n",
    "| `assertIsNotNone(x)`                    | `x is not None`                   |\n",
    "| `assertIn(a, b)`                        | `a in b`                          |\n",
    "| `assertNotIn(a, b)`                     | `a not in b`                      |\n",
    "| `assertIsInstance(a, b)`                | `isinstance(a, b)`                |\n",
    "| `assertNotIsInstance(a, b)`             | `not isinstance(a, b)`            |\n",
    "| `assertRaises(exc, fun, *args, **kwds)` | `fun(*args, **kwds)` raises `exc` |\n",
    "\n",
    "\n",
    "Test fixtures represent the initial set-up needed before each test method or\n",
    "before all the tests in a test case. This can be achieved by using the special\n",
    "`setUp` method that will be called before every test run. Similarly, we can \n",
    "provide a `tearDown()` method that tidies up after the test method has been\n",
    "run.\n",
    "\n",
    "\n",
    "```python\n",
    "import unittest\n",
    "\n",
    "class WidgetTestCase(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.widget = Widget('The widget')\n",
    "\n",
    "    def test_default_widget_size(self):\n",
    "        self.assertEqual(self.widget.size(), (50,50),\n",
    "                         'incorrect default size')\n",
    "\n",
    "    def test_widget_resize(self):\n",
    "        self.widget.resize(100,150)\n",
    "        self.assertEqual(self.widget.size(), (100,150),\n",
    "                         'wrong size after resize')\n",
    "\n",
    "    def tearDown(self):\n",
    "        self.widget.dispose()\n",
    "```\n",
    "\n",
    "### Command-Line Interface\n",
    "\n",
    "The unittest module can be used from the command line to run tests from modules, classes or even individual test methods:\n",
    "\n",
    "```shell\n",
    "python -m unittest test_module1 test_module2\n",
    "python -m unittest test_module.TestClass\n",
    "python -m unittest test_module.TestClass.test_method\n",
    "```\n",
    "\n",
    "If you with to stop test run on the first error or failure, run the tests with\n",
    "`-f, --failfast` command-line option:\n",
    "\n",
    "```shell\n",
    "python -m unittest -f\n",
    "python -m unittest --failfast\n",
    "```\n",
    "\n",
    "For a list of all the command-line options:\n",
    "\n",
    "```shell\n",
    "python -m unittest -h\n",
    "```\n",
    "\n",
    "### Test Suites\n",
    "\n",
    "It is recommended that you use `TestCase` implementations to group tests together according to the features they test. `unittest` provides another mechanism for grouping tests: **the test suite**, represented by `unittest`’s `TestSuite` class. In most cases, calling unittest.main() will do the right thing and collect all the module’s test cases for you and execute them.\n",
    "\n",
    "However, should you want to customize the building of your test suite, you can do it yourself:\n",
    "\n",
    "```python\n",
    "def suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTest(WidgetTestCase('test_default_widget_size'))\n",
    "    suite.addTest(WidgetTestCase('test_widget_resize'))\n",
    "    return suite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite())\n",
    "```\n",
    "\n",
    "### Subtests\n",
    "\n",
    "When there are very small differences among your tests, for instance some parameters, unittest allows you to distinguish them inside the body of a test method using the `subTest()` context manager.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f67514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_even (__main__.NumbersTest.test_even)\n",
      "Test that numbers between 0 and 5 are all even. ... \n",
      "  test_even (__main__.NumbersTest.test_even) [Test failed for i = 1]\n",
      "Test that numbers between 0 and 5 are all even. ... FAIL\n",
      "  test_even (__main__.NumbersTest.test_even) [Test failed for i = 3]\n",
      "Test that numbers between 0 and 5 are all even. ... FAIL\n",
      "  test_even (__main__.NumbersTest.test_even) [Test failed for i = 5]\n",
      "Test that numbers between 0 and 5 are all even. ... FAIL\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest.test_even) [Test failed for i = 1]\n",
      "Test that numbers between 0 and 5 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\", line 12, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest.test_even) [Test failed for i = 3]\n",
      "Test that numbers between 0 and 5 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\", line 12, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_even (__main__.NumbersTest.test_even) [Test failed for i = 5]\n",
      "Test that numbers between 0 and 5 are all even.\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\", line 12, in test_even\n",
      "    self.assertEqual(i % 2, 0)\n",
      "AssertionError: 1 != 0\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.004s\n",
      "\n",
      "FAILED (failures=3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1035a8b00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class NumbersTest(unittest.TestCase):\n",
    "\n",
    "    def test_even(self):\n",
    "        \"\"\"\n",
    "        Test that numbers between 0 and 5 are all even.\n",
    "        \"\"\"\n",
    "        for i in range(0, 6):\n",
    "            with self.subTest(f\"Test failed for i = {i}\"):\n",
    "                self.assertEqual(i % 2, 0)\n",
    " \n",
    "\n",
    "unittest.main(argv=['-k', 'NumbersTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2398c",
   "metadata": {},
   "source": [
    "### Skipping tests and expected failures\n",
    "\n",
    "Unittest supports skipping individual test methods and even whole classes of tests. In addition, it supports marking a test as an “expected failure,” a test that is broken and will fail, but shouldn’t be counted as a failure on a TestResult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12a6b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_fail (__main__.MyTestCase.test_fail) ... expected failure\n",
      "test_format (__main__.MyTestCase.test_format) ... skipped 'not supported in this library version'\n",
      "test_maybe_skipped (__main__.MyTestCase.test_maybe_skipped) ... skipped 'external resource not available'\n",
      "test_nothing (__main__.MyTestCase.test_nothing) ... skipped 'demonstrating skipping'\n",
      "test_windows_support (__main__.MyTestCase.test_windows_support) ... skipped 'requires Windows'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.003s\n",
      "\n",
      "OK (skipped=4, expected failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1035c4920>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import unittest\n",
    "\n",
    "VERSION = (1, 2)\n",
    "\n",
    "def external_resource_available():\n",
    "    return False\n",
    "\n",
    "\n",
    "class MyTestCase(unittest.TestCase):\n",
    "\n",
    "    @unittest.skip(\"demonstrating skipping\")\n",
    "    def test_nothing(self):\n",
    "        self.fail(\"shouldn't happen\")\n",
    "\n",
    "    @unittest.skipIf(VERSION < (1, 3),\n",
    "                     \"not supported in this library version\")\n",
    "    def test_format(self):\n",
    "        # Tests that work for only a certain version of the library.\n",
    "        pass\n",
    "\n",
    "    @unittest.skipUnless(sys.platform.startswith(\"win\"), \"requires Windows\")\n",
    "    def test_windows_support(self):\n",
    "        # windows specific testing code\n",
    "        pass\n",
    "\n",
    "    def test_maybe_skipped(self):\n",
    "        if not external_resource_available():\n",
    "            self.skipTest(\"external resource not available\")\n",
    "        # test code that depends on the external resource\n",
    "        pass\n",
    "    \n",
    "    @unittest.expectedFailure\n",
    "    def test_fail(self):\n",
    "        self.assertEqual(1, 0, \"broken\")\n",
    "    \n",
    "    \n",
    "unittest.main(argv=['-k', 'MyTestCase'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a6d85c",
   "metadata": {},
   "source": [
    "### Exercises 1\n",
    "\n",
    "1. Write tests for `Employee.raise_salary()` using `unittest`. Consider all significant cases (raise with valid/invalid percent). Use subtests to test all valid values. Use a fixture for the `Employee` object.\n",
    "\n",
    "## 2. Testing frameworks: `pytest`\n",
    "\n",
    "`pytest` is a 3rd party library built as an alternative to Standard Library's `unittest`.\n",
    "\n",
    "`pytest` supports execution of unittest test cases, but the real advantage of `pytest` comes by writing `pytest` test cases. `pytest` test cases are a series of functions in a Python file starting with the name `test_`.\n",
    "\n",
    "`pytest` has some other great features:\n",
    "\n",
    "* Tests are expressive and readable — no boilerplate code required\n",
    "* Support for the built-in `assert` statement instead of using special `self.assert*()` methods\n",
    "* Support for filtering for test cases\n",
    "* Ability to rerun from the last failing test\n",
    "* Marks and parametrized tests\n",
    "* Modular fixtures\n",
    "* An ecosystem of hundreds of plugins to extend the functionality\n",
    "\n",
    "### Installation\n",
    "\n",
    "Because it is a 3rd party library, you should install it using `pip`:\n",
    "\n",
    "```\n",
    "pip install pytest\n",
    "```\n",
    "\n",
    "### Writing a simple test\n",
    "\n",
    "Tests in `pytest` are simple functions. Assertions are done using `assert` statement.\n",
    "\n",
    "In order to execute pytests inside Jupyter Notebook, we're going to use a package called `ipytest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a34ee021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytest\n",
      "  Downloading pytest-8.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting iniconfig (from pytest)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: packaging in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from pytest) (24.0)\n",
      "Collecting pluggy<2.0,>=1.5 (from pytest)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Downloading pytest-8.2.2-py3-none-any.whl (339 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m339.9/339.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Installing collected packages: pluggy, iniconfig, pytest\n",
      "Successfully installed iniconfig-2.0.0 pluggy-1.5.0 pytest-8.2.2\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.14.2-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: ipython in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipytest) (8.25.0)\n",
      "Requirement already satisfied: packaging in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipytest) (24.0)\n",
      "Requirement already satisfied: pytest>=5.4 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipytest) (8.2.2)\n",
      "Requirement already satisfied: iniconfig in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=1.5 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from pytest>=5.4->ipytest) (1.5.0)\n",
      "Requirement already satisfied: decorator in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (3.0.46)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from ipython->ipytest) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython->ipytest) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->ipytest) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from stack-data->ipython->ipytest) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from stack-data->ipython->ipytest) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from stack-data->ipython->ipytest) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/iulia/PycharmProjects/python-advanced-course/.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython->ipytest) (1.16.0)\n",
      "Downloading ipytest-0.14.2-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: ipytest\n",
      "Successfully installed ipytest-0.14.2\n"
     ]
    }
   ],
   "source": [
    "import sys                                                                                                                                                                                                  \n",
    "!{sys.executable} -m pip install pytest\n",
    "!{sys.executable} -m pip install ipytest\n",
    "\n",
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82529bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[33mx\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                     [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________________ NumbersTest.test_even _______________________________________\u001b[0m\n",
      "\n",
      "self = <__main__.NumbersTest testMethod=test_even>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_even\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Test that numbers between 0 and 5 are all even.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.subTest(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest failed for i = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[96mself\u001b[39;49;00m.assertEqual(i % \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: 1 != 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m__________________________________________ test_func_fail __________________________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_func_fail\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m func(\u001b[94m3\u001b[39;49;00m) == \u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 4 == 5\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 4 = func(3)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/2446215452.py\u001b[0m:9: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mNumbersTest::test_even\u001b[0m - AssertionError: 1 != 0\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mtest_func_fail\u001b[0m - assert 4 == 5\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def test_func_pass():\n",
    "    assert func(3) == 4\n",
    "    \n",
    "def test_func_fail():\n",
    "    assert func(3) == 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661f00c",
   "metadata": {},
   "source": [
    "### Assert that an exception is raised\n",
    "\n",
    "`pytest` implements a helper function that can be used with the `with` statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fe3b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[33mx\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                      [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________________ NumbersTest.test_even _______________________________________\u001b[0m\n",
      "\n",
      "self = <__main__.NumbersTest testMethod=test_even>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_even\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Test that numbers between 0 and 5 are all even.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.subTest(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest failed for i = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[96mself\u001b[39;49;00m.assertEqual(i % \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: 1 != 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\u001b[0m:12: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mNumbersTest::test_even\u001b[0m - AssertionError: 1 != 0\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "def func():\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "def test_raises():\n",
    "    with pytest.raises(ValueError):\n",
    "        func()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7413780",
   "metadata": {},
   "source": [
    "## Command-line interface\n",
    "\n",
    "`pytest`'s command line interface is more powerful than `unittest`s.\n",
    "\n",
    "Running `pytest --help` will give you a list of all the arguments throught which the behaviour of the testing framework can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20fee8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [options] [file_or_dir] [file_or_dir] [...]\n",
      "\n",
      "positional arguments:\n",
      "  file_or_dir\n",
      "\n",
      "general:\n",
      "  -k EXPRESSION         Only run tests which match the given substring\n",
      "                        expression. An expression is a Python evaluable\n",
      "                        expression where all names are substring-matched against\n",
      "                        test names and their parent classes. Example: -k\n",
      "                        'test_method or test_other' matches all test functions\n",
      "                        and classes whose name contains 'test_method' or\n",
      "                        'test_other', while -k 'not test_method' matches those\n",
      "                        that don't contain 'test_method' in their names. -k 'not\n",
      "                        test_method and not test_other' will eliminate the\n",
      "                        matches. Additionally keywords are matched to classes\n",
      "                        and functions containing extra names in their\n",
      "                        'extra_keyword_matches' set, as well as functions which\n",
      "                        have names assigned directly to them. The matching is\n",
      "                        case-insensitive.\n",
      "  -m MARKEXPR           Only run tests matching given mark expression. For\n",
      "                        example: -m 'mark1 and not mark2'.\n",
      "  --markers             show markers (builtin, plugin and per-project ones).\n",
      "  -x, --exitfirst       Exit instantly on first error or failed test\n",
      "  --fixtures, --funcargs\n",
      "                        Show available fixtures, sorted by plugin appearance\n",
      "                        (fixtures with leading '_' are only shown with '-v')\n",
      "  --fixtures-per-test   Show fixtures per test\n",
      "  --pdb                 Start the interactive Python debugger on errors or\n",
      "                        KeyboardInterrupt\n",
      "  --pdbcls=modulename:classname\n",
      "                        Specify a custom interactive Python debugger for use\n",
      "                        with --pdb.For example:\n",
      "                        --pdbcls=IPython.terminal.debugger:TerminalPdb\n",
      "  --trace               Immediately break when running each test\n",
      "  --capture=method      Per-test capturing method: one of fd|sys|no|tee-sys\n",
      "  -s                    Shortcut for --capture=no\n",
      "  --runxfail            Report the results of xfail tests as if they were not\n",
      "                        marked\n",
      "  --lf, --last-failed   Rerun only the tests that failed at the last run (or all\n",
      "                        if none failed)\n",
      "  --ff, --failed-first  Run all tests, but run the last failures first. This may\n",
      "                        re-order tests and thus lead to repeated fixture\n",
      "                        setup/teardown.\n",
      "  --nf, --new-first     Run tests from new files first, then the rest of the\n",
      "                        tests sorted by file mtime\n",
      "  --cache-show=[CACHESHOW]\n",
      "                        Show cache contents, don't perform collection or tests.\n",
      "                        Optional argument: glob (default: '*').\n",
      "  --cache-clear         Remove all cache contents at start of test run\n",
      "  --lfnf={all,none}, --last-failed-no-failures={all,none}\n",
      "                        With ``--lf``, determines whether to execute tests when\n",
      "                        there are no previously (known) failures or when no\n",
      "                        cached ``lastfailed`` data was found. ``all`` (the\n",
      "                        default) runs the full test suite again. ``none`` just\n",
      "                        emits a message about no known failures and exits\n",
      "                        successfully.\n",
      "  --sw, --stepwise      Exit on test failure and continue from last failing test\n",
      "                        next time\n",
      "  --sw-skip, --stepwise-skip\n",
      "                        Ignore the first failing test but stop on the next\n",
      "                        failing test. Implicitly enables --stepwise.\n",
      "\n",
      "Reporting:\n",
      "  --durations=N         Show N slowest setup/test durations (N=0 for all)\n",
      "  --durations-min=N     Minimal duration in seconds for inclusion in slowest\n",
      "                        list. Default: 0.005.\n",
      "  -v, --verbose         Increase verbosity\n",
      "  --no-header           Disable header\n",
      "  --no-summary          Disable summary\n",
      "  -q, --quiet           Decrease verbosity\n",
      "  --verbosity=VERBOSE   Set verbosity. Default: 0.\n",
      "  -r chars              Show extra test summary info as specified by chars:\n",
      "                        (f)ailed, (E)rror, (s)kipped, (x)failed, (X)passed,\n",
      "                        (p)assed, (P)assed with output, (a)ll except passed\n",
      "                        (p/P), or (A)ll. (w)arnings are enabled by default (see\n",
      "                        --disable-warnings), 'N' can be used to reset the list.\n",
      "                        (default: 'fE').\n",
      "  --disable-warnings, --disable-pytest-warnings\n",
      "                        Disable warnings summary\n",
      "  -l, --showlocals      Show locals in tracebacks (disabled by default)\n",
      "  --no-showlocals       Hide locals in tracebacks (negate --showlocals passed\n",
      "                        through addopts)\n",
      "  --tb=style            Traceback print mode (auto/long/short/line/native/no)\n",
      "  --show-capture={no,stdout,stderr,log,all}\n",
      "                        Controls how captured stdout/stderr/log is shown on\n",
      "                        failed tests. Default: all.\n",
      "  --full-trace          Don't cut any tracebacks (default is to cut)\n",
      "  --color=color         Color terminal output (yes/no/auto)\n",
      "  --code-highlight={yes,no}\n",
      "                        Whether code should be highlighted (only if --color is\n",
      "                        also enabled). Default: yes.\n",
      "  --pastebin=mode       Send failed|all info to bpaste.net pastebin service\n",
      "  --junit-xml=path      Create junit-xml style report file at given path\n",
      "  --junit-prefix=str    Prepend prefix to classnames in junit-xml output\n",
      "\n",
      "pytest-warnings:\n",
      "  -W PYTHONWARNINGS, --pythonwarnings=PYTHONWARNINGS\n",
      "                        Set which warnings to report, see -W option of Python\n",
      "                        itself\n",
      "  --maxfail=num         Exit after first num failures or errors\n",
      "  --strict-config       Any warnings encountered while parsing the `pytest`\n",
      "                        section of the configuration file raise errors\n",
      "  --strict-markers      Markers not registered in the `markers` section of the\n",
      "                        configuration file raise errors\n",
      "  --strict              (Deprecated) alias to --strict-markers\n",
      "  -c FILE, --config-file=FILE\n",
      "                        Load configuration from `FILE` instead of trying to\n",
      "                        locate one of the implicit configuration files.\n",
      "  --continue-on-collection-errors\n",
      "                        Force test execution even if collection errors occur\n",
      "  --rootdir=ROOTDIR     Define root directory for tests. Can be relative path:\n",
      "                        'root_dir', './root_dir', 'root_dir/another_dir/';\n",
      "                        absolute path: '/home/user/root_dir'; path with\n",
      "                        variables: '$HOME/root_dir'.\n",
      "\n",
      "collection:\n",
      "  --collect-only, --co  Only collect tests, don't execute them\n",
      "  --pyargs              Try to interpret all arguments as Python packages\n",
      "  --ignore=path         Ignore path during collection (multi-allowed)\n",
      "  --ignore-glob=path    Ignore path pattern during collection (multi-allowed)\n",
      "  --deselect=nodeid_prefix\n",
      "                        Deselect item (via node id prefix) during collection\n",
      "                        (multi-allowed)\n",
      "  --confcutdir=dir      Only load conftest.py's relative to specified dir\n",
      "  --noconftest          Don't load any conftest.py files\n",
      "  --keep-duplicates     Keep duplicate tests\n",
      "  --collect-in-virtualenv\n",
      "                        Don't ignore tests in a local virtualenv directory\n",
      "  --import-mode={prepend,append,importlib}\n",
      "                        Prepend/append to sys.path when importing test modules\n",
      "                        and conftest files. Default: prepend.\n",
      "  --doctest-modules     Run doctests in all .py modules\n",
      "  --doctest-report={none,cdiff,ndiff,udiff,only_first_failure}\n",
      "                        Choose another output format for diffs on doctest\n",
      "                        failure\n",
      "  --doctest-glob=pat    Doctests file matching pattern, default: test*.txt\n",
      "  --doctest-ignore-import-errors\n",
      "                        Ignore doctest collection errors\n",
      "  --doctest-continue-on-failure\n",
      "                        For a given doctest, continue to run after the first\n",
      "                        failure\n",
      "\n",
      "test session debugging and configuration:\n",
      "  --basetemp=dir        Base temporary directory for this test run. (Warning:\n",
      "                        this directory is removed if it exists.)\n",
      "  -V, --version         Display pytest version and information about plugins.\n",
      "                        When given twice, also display information about\n",
      "                        plugins.\n",
      "  -h, --help            Show help message and configuration info\n",
      "  -p name               Early-load given plugin module name or entry point\n",
      "                        (multi-allowed). To avoid loading of plugins, use the\n",
      "                        `no:` prefix, e.g. `no:doctest`.\n",
      "  --trace-config        Trace considerations of conftest.py files\n",
      "  --debug=[DEBUG_FILE_NAME]\n",
      "                        Store internal tracing debug information in this log\n",
      "                        file. This file is opened with 'w' and truncated as a\n",
      "                        result, care advised. Default: pytestdebug.log.\n",
      "  -o OVERRIDE_INI, --override-ini=OVERRIDE_INI\n",
      "                        Override ini option with \"option=value\" style, e.g. `-o\n",
      "                        xfail_strict=True -o cache_dir=cache`.\n",
      "  --assert=MODE         Control assertion debugging tools.\n",
      "                        'plain' performs no assertion debugging.\n",
      "                        'rewrite' (the default) rewrites assert statements in\n",
      "                        test modules on import to provide assert expression\n",
      "                        information.\n",
      "  --setup-only          Only setup fixtures, do not execute tests\n",
      "  --setup-show          Show setup of fixtures while executing tests\n",
      "  --setup-plan          Show what fixtures and tests would be executed but don't\n",
      "                        execute anything\n",
      "\n",
      "logging:\n",
      "  --log-level=LEVEL     Level of messages to catch/display. Not set by default,\n",
      "                        so it depends on the root/parent log handler's effective\n",
      "                        level, where it is \"WARNING\" by default.\n",
      "  --log-format=LOG_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-date-format=LOG_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-cli-level=LOG_CLI_LEVEL\n",
      "                        CLI logging level\n",
      "  --log-cli-format=LOG_CLI_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-cli-date-format=LOG_CLI_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-file=LOG_FILE   Path to a file when logging will be written to\n",
      "  --log-file-mode={w,a}\n",
      "                        Log file open mode\n",
      "  --log-file-level=LOG_FILE_LEVEL\n",
      "                        Log file logging level\n",
      "  --log-file-format=LOG_FILE_FORMAT\n",
      "                        Log format used by the logging module\n",
      "  --log-file-date-format=LOG_FILE_DATE_FORMAT\n",
      "                        Log date format used by the logging module\n",
      "  --log-auto-indent=LOG_AUTO_INDENT\n",
      "                        Auto-indent multiline messages passed to the logging\n",
      "                        module. Accepts true|on, false|off or an integer.\n",
      "  --log-disable=LOGGER_DISABLE\n",
      "                        Disable a logger by name. Can be passed multiple times.\n",
      "\n",
      "[pytest] ini-options in the first pytest.ini|tox.ini|setup.cfg|pyproject.toml file found:\n",
      "\n",
      "  markers (linelist):   Register new markers for test functions\n",
      "  empty_parameter_set_mark (string):\n",
      "                        Default marker for empty parametersets\n",
      "  norecursedirs (args): Directory patterns to avoid for recursion\n",
      "  testpaths (args):     Directories to search for tests when no files or\n",
      "                        directories are given on the command line\n",
      "  filterwarnings (linelist):\n",
      "                        Each line specifies a pattern for\n",
      "                        warnings.filterwarnings. Processed after\n",
      "                        -W/--pythonwarnings.\n",
      "  consider_namespace_packages (bool):\n",
      "                        Consider namespace packages when resolving module names\n",
      "                        during import\n",
      "  usefixtures (args):   List of default fixtures to be used with this project\n",
      "  python_files (args):  Glob-style file patterns for Python test module\n",
      "                        discovery\n",
      "  python_classes (args):\n",
      "                        Prefixes or glob names for Python test class discovery\n",
      "  python_functions (args):\n",
      "                        Prefixes or glob names for Python test function and\n",
      "                        method discovery\n",
      "  disable_test_id_escaping_and_forfeit_all_rights_to_community_support (bool):\n",
      "                        Disable string escape non-ASCII characters, might cause\n",
      "                        unwanted side effects(use at your own risk)\n",
      "  console_output_style (string):\n",
      "                        Console output: \"classic\", or with additional progress\n",
      "                        information (\"progress\" (percentage) | \"count\" |\n",
      "                        \"progress-even-when-capture-no\" (forces progress even\n",
      "                        when capture=no)\n",
      "  verbosity_test_cases (string):\n",
      "                        Specify a verbosity level for test case execution,\n",
      "                        overriding the main level. Higher levels will provide\n",
      "                        more detailed information about each test case executed.\n",
      "  xfail_strict (bool):  Default for the strict parameter of xfail markers when\n",
      "                        not given explicitly (default: False)\n",
      "  tmp_path_retention_count (string):\n",
      "                        How many sessions should we keep the `tmp_path`\n",
      "                        directories, according to `tmp_path_retention_policy`.\n",
      "  tmp_path_retention_policy (string):\n",
      "                        Controls which directories created by the `tmp_path`\n",
      "                        fixture are kept around, based on test outcome.\n",
      "                        (all/failed/none)\n",
      "  enable_assertion_pass_hook (bool):\n",
      "                        Enables the pytest_assertion_pass hook. Make sure to\n",
      "                        delete any previously generated pyc cache files.\n",
      "  verbosity_assertions (string):\n",
      "                        Specify a verbosity level for assertions, overriding the\n",
      "                        main level. Higher levels will provide more detailed\n",
      "                        explanation when an assertion fails.\n",
      "  junit_suite_name (string):\n",
      "                        Test suite name for JUnit report\n",
      "  junit_logging (string):\n",
      "                        Write captured log messages to JUnit report: one of\n",
      "                        no|log|system-out|system-err|out-err|all\n",
      "  junit_log_passing_tests (bool):\n",
      "                        Capture log information for passing tests to JUnit\n",
      "                        report:\n",
      "  junit_duration_report (string):\n",
      "                        Duration time to report: one of total|call\n",
      "  junit_family (string):\n",
      "                        Emit XML for schema: one of legacy|xunit1|xunit2\n",
      "  doctest_optionflags (args):\n",
      "                        Option flags for doctests\n",
      "  doctest_encoding (string):\n",
      "                        Encoding used for doctest files\n",
      "  cache_dir (string):   Cache directory path\n",
      "  log_level (string):   Default value for --log-level\n",
      "  log_format (string):  Default value for --log-format\n",
      "  log_date_format (string):\n",
      "                        Default value for --log-date-format\n",
      "  log_cli (bool):       Enable log display during test run (also known as \"live\n",
      "                        logging\")\n",
      "  log_cli_level (string):\n",
      "                        Default value for --log-cli-level\n",
      "  log_cli_format (string):\n",
      "                        Default value for --log-cli-format\n",
      "  log_cli_date_format (string):\n",
      "                        Default value for --log-cli-date-format\n",
      "  log_file (string):    Default value for --log-file\n",
      "  log_file_mode (string):\n",
      "                        Default value for --log-file-mode\n",
      "  log_file_level (string):\n",
      "                        Default value for --log-file-level\n",
      "  log_file_format (string):\n",
      "                        Default value for --log-file-format\n",
      "  log_file_date_format (string):\n",
      "                        Default value for --log-file-date-format\n",
      "  log_auto_indent (string):\n",
      "                        Default value for --log-auto-indent\n",
      "  pythonpath (paths):   Add paths to sys.path\n",
      "  faulthandler_timeout (string):\n",
      "                        Dump the traceback of all threads if a test takes more\n",
      "                        than TIMEOUT seconds to finish\n",
      "  addopts (args):       Extra command line options\n",
      "  minversion (string):  Minimally required pytest version\n",
      "  required_plugins (args):\n",
      "                        Plugins that must be present for pytest to run\n",
      "\n",
      "Environment variables:\n",
      "  PYTEST_ADDOPTS           Extra command line options\n",
      "  PYTEST_PLUGINS           Comma-separated plugins to load during startup\n",
      "  PYTEST_DISABLE_PLUGIN_AUTOLOAD Set to disable plugin auto-loading\n",
      "  PYTEST_DEBUG             Set to enable debug tracing of pytest's internals\n",
      "\n",
      "\n",
      "to see available markers type: pytest --markers\n",
      "to see available fixtures type: pytest --fixtures\n",
      "(shown according to specified file_or_dir or current dir if not specified; fixtures with leading '_' are only shown with the '-v' option\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pytest --help\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ead4d",
   "metadata": {},
   "source": [
    "### Invocation\n",
    "\n",
    "You can invoke testing through the Python interpreter from the command line:\n",
    "\n",
    "```\n",
    "python -m pytest [...]\n",
    "```\n",
    "\n",
    "This is almost equivalent to invoking the command line script `pytest [...]` directly, except that calling via python will also add the current directory to `sys.path`.\n",
    "\n",
    "### Running specific tests\n",
    "\n",
    "Pytest supports several ways to run and select tests from the command-line.\n",
    "\n",
    "Run tests in a module\n",
    "```\n",
    "pytest test_mod.py\n",
    "```\n",
    "Run tests in a directory\n",
    "```\n",
    "pytest testing/\n",
    "```\n",
    "Run tests by keyword expressions\n",
    "```\n",
    "pytest -k \"MyClass and not method\"\n",
    "```\n",
    "This will run tests which contain names that match the given string expression (case-insensitive), which can include Python operators that use filenames, class names and function names as variables. The example above will run `TestMyClass.test_something` but not `TestMyClass.test_method_simple`.\n",
    "\n",
    "#### Run tests by node ids\n",
    "\n",
    "Each collected test is assigned a unique nodeid which consist of the module filename followed by specifiers like class names, function names and parameters from parametrization, separated by :: characters.\n",
    "\n",
    "To run a specific test within a module:\n",
    "\n",
    "```\n",
    "pytest test_mod.py::test_func\n",
    "```\n",
    "Another example specifying a test method in the command line:\n",
    "```\n",
    "pytest test_mod.py::TestClass::test_method\n",
    "```\n",
    "\n",
    "### Stopping on failures\n",
    "\n",
    "To stop the testing process after the first (N) failures:\n",
    "\n",
    "```\n",
    "pytest -x           # stop after first failure\n",
    "pytest --maxfail=2  # stop after two failures\n",
    "```\n",
    "\n",
    "Read more about `pytest`'s command-line interface [here](https://docs.pytest.org/en/6.2.x/usage.html).\n",
    "\n",
    "## Fixtures\n",
    "\n",
    "`pytest` fixtures offer dramatic improvements over the classic xUnit style of setup/teardown functions:\n",
    "\n",
    "* fixtures have explicit names and are activated by declaring their use from test functions, modules, classes or whole projects.\n",
    "* fixtures are implemented in a modular manner, as each fixture name triggers a fixture function which can itself use other fixtures.\n",
    "* fixture management scales from simple unit to complex functional testing, allowing to parametrize fixtures and tests according to configuration and component options, or to re-use fixtures across function, class, module or whole test session scopes.\n",
    "* teardown logic can be easily, and safely managed, no matter how many fixtures are used, without the need to carefully handle errors by hand or micromanage the order that cleanup steps are added.\n",
    "\n",
    "We can tell pytest that a particular function is a fixture by decorating it with `@pytest.fixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86446af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[33mx\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                                                      [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________________ NumbersTest.test_even _______________________________________\u001b[0m\n",
      "\n",
      "self = <__main__.NumbersTest testMethod=test_even>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_even\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Test that numbers between 0 and 5 are all even.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.subTest(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest failed for i = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[96mself\u001b[39;49;00m.assertEqual(i % \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: 1 != 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\u001b[0m:12: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mNumbersTest::test_even\u001b[0m - AssertionError: 1 != 0\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "class Fruit:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.name == other.name\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fruit():\n",
    "    return Fruit(\"apple\")\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def fruit_basket(my_fruit):\n",
    "    return [Fruit(\"banana\"), my_fruit]\n",
    "\n",
    "\n",
    "def test_my_fruit_in_basket(my_fruit, fruit_basket):\n",
    "    assert my_fruit in fruit_basket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04902ea",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "By using the `pytest.mark` helper you can easily set metadata on your test functions. Markers can be built-in or user-defined. You can list all the markers, including built-in and custom, using the CLI `pytest --markers`.\n",
    "\n",
    "Here are some of the builtin markers:\n",
    "\n",
    "* `usefixtures` - use fixtures on a test function or class\n",
    "* `filterwarnings` - filter certain warnings of a test function\n",
    "* `skip` - always skip a test function\n",
    "* `skipif` - skip a test function if a certain condition is met\n",
    "* `xfail` - produce an “expected failure” outcome if a certain condition is met\n",
    "* `parametrize` - perform multiple calls to the same test function.\n",
    "\n",
    "It’s easy to create custom markers or to apply markers to whole test classes or modules. Those markers can be used by plugins, and also are commonly used to select tests on the command-line with the -m option.\n",
    "\n",
    "### Registering marks\n",
    "\n",
    "You can register custom marks in your `pytest.ini` file like this:\n",
    "\n",
    "```ini\n",
    "[pytest]\n",
    "markers =\n",
    "    slow: marks tests as slow (deselect with '-m \"not slow\"')\n",
    "    serial\n",
    "```\n",
    "\n",
    "### Marking tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d1e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq -m slow\n",
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "def pytest_configure(config):\n",
    "    config.addinivalue_line(\n",
    "        \"markers\", \"slow: marks tests as slow\"\n",
    "    )\n",
    "\n",
    "def func():\n",
    "    pass\n",
    "\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_func():\n",
    "    assert func() is None\n",
    "\n",
    "def test_serial():\n",
    "    assert 1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699e154",
   "metadata": {},
   "source": [
    "## Parametrized tests\n",
    "\n",
    "The builtin `pytest.mark.parametrize` decorator enables parametrization of arguments for a test function. Here is a typical example of a test function that implements checking that a certain input leads to an expected output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ffbba47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mF\u001b[0m\u001b[33mx\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                                                    [100%]\u001b[0m\n",
      "============================================= FAILURES =============================================\n",
      "\u001b[31m\u001b[1m______________________________________ NumbersTest.test_even _______________________________________\u001b[0m\n",
      "\n",
      "self = <__main__.NumbersTest testMethod=test_even>\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_even\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m\n",
      "    \u001b[33m    Test that numbers between 0 and 5 are all even.\u001b[39;49;00m\n",
      "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[94m0\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      "            \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.subTest(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTest failed for i = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mi\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
      ">               \u001b[96mself\u001b[39;49;00m.assertEqual(i % \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE               AssertionError: 1 != 0\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/1847049937.py\u001b[0m:12: AssertionError\n",
      "\u001b[31m\u001b[1m________________________________________ test_eval[6*9-42] _________________________________________\u001b[0m\n",
      "\n",
      "test_input = '6*9', expected = 42\n",
      "\n",
      "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mtest_input,expected\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [(\u001b[33m\"\u001b[39;49;00m\u001b[33m3+5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), (\u001b[33m\"\u001b[39;49;00m\u001b[33m2+4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m6\u001b[39;49;00m), (\u001b[33m\"\u001b[39;49;00m\u001b[33m6*9\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94m42\u001b[39;49;00m)])\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_eval\u001b[39;49;00m(test_input, expected):\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m \u001b[96meval\u001b[39;49;00m(test_input) == expected\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 54 == 42\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 54 = eval('6*9')\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31m/var/folders/s2/_752pzb50tg00mv9ggtr6p8c0000gn/T/ipykernel_46693/2167493762.py\u001b[0m:6: AssertionError\n",
      "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mNumbersTest::test_even\u001b[0m - AssertionError: 1 != 0\n",
      "\u001b[31mFAILED\u001b[0m t_7497a9e926e34c6fa9f6fd9a453c1b90.py::\u001b[1mtest_eval[6*9-42]\u001b[0m - AssertionError: assert 54 == 42\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
    "def test_eval(test_input, expected):\n",
    "    assert eval(test_input) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d4292-0e52-49a2-982b-98addbee1d46",
   "metadata": {},
   "source": [
    "### Exercises 2\n",
    "\n",
    "1. Run all existing tests with `pytest`.\n",
    "2. Create `search_term` tests using `pytest`. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
